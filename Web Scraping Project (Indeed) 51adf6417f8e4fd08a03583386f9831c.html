<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Web Scraping Project (Indeed)</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="51adf641-7f8e-4fd0-8a03-583386f9831c" class="page sans"><header><h1 class="page-title">Web Scraping Project (Indeed)</h1><p class="page-description"></p></header><div class="page-body"><p id="4ad80de1-cc1a-4578-a1bf-874acbbd5e22" class="">ในโปรเจคนี้เราต้องการที่จะดึงข้อมูลการเปิดรับสมัครพนักงานภายใต้การค้นหา “Data Analyst” จากเว็บไซต์หางานยอดนิยมอย่าง <a href="http://Indeed.com">Indeed.com</a> ด้วยภาษา <strong>Python (Selenium </strong><mark class="highlight-gray"><strong>webdriver</strong></mark><strong> + BeautifulSoup)</strong></p><p id="ccfea31d-c73a-4c6a-9994-d3a6493e36e2" class="">
</p><p id="caa5b152-dc02-494b-823c-ce46e04a81a1" class="">โดยต้องการรายละเอียดดังนี้</p><ul id="9cc326b8-50dd-4282-9e03-7e8888ed0755" class="bulleted-list"><li style="list-style-type:disc">Title (ชื่อตำแหน่งงาน)</li></ul><ul id="dd67b8d1-842a-4951-b73d-3609b75c8e29" class="bulleted-list"><li style="list-style-type:disc">Company (ชื่อบริษัท)</li></ul><ul id="79197b99-8192-42fb-8860-442addab5f75" class="bulleted-list"><li style="list-style-type:disc">Location (ตำแหน่งที่ตั้งของบริษัท)</li></ul><ul id="37b9f2bb-be99-4b45-b342-fd57855d331d" class="bulleted-list"><li style="list-style-type:disc">Salary (เงินเดือน)</li></ul><ul id="4e31418f-3ef9-4ffc-bbbc-ffb594863c2a" class="bulleted-list"><li style="list-style-type:disc">Date posted (จำนวนวันตั้งแต่วันที่บริษัทประกาศรับสมัคร)</li></ul><ul id="e63472b8-15d8-4837-b7b5-65a46d242fb3" class="bulleted-list"><li style="list-style-type:disc">Summary (คำอธิบายเกี่ยวกับงานคร่าวๆ)</li></ul><p id="5935d4a1-d500-4ddc-b30d-0bdbe5395cdc" class="">
</p><p id="4c113a20-d4ff-4ea4-ac0f-4d502f89da24" class=""><mark class="highlight-gray">โดยอ้างอิงจากเว็บไซต์</mark><strong><mark class="highlight-gray"> https://th.indeed.com</mark></strong></p><figure id="84531068-ae2b-4da6-bfa3-8201d6f88341" class="image"><a href="Web%20Scraping%20Project%20(Indeed)%2051adf6417f8e4fd08a03583386f9831c/Untitled.png"><img style="width:864px" src="Web%20Scraping%20Project%20(Indeed)%2051adf6417f8e4fd08a03583386f9831c/Untitled.png"/></a></figure><p id="41651e0d-fe84-4e76-bf8d-f8d87381c32a" class="">
</p><p id="e148c4db-d27b-45bd-87f8-eb7e02142cae" class="">เริ่มต้นด้วย Import Library ที่จำเป็นในการทำ Web Scraping กันก่อน โดยจะมี </p><ul id="9ee1166a-5d92-41f4-80fa-d8afdb259fcd" class="bulleted-list"><li style="list-style-type:disc">Selenium - Webdriver ใช้สำหรับจำลองการรันเว็บไซต์</li></ul><ul id="9683e983-4636-4d11-8e73-1b4603d30a8b" class="bulleted-list"><li style="list-style-type:disc">BeautifulSoup </li></ul><ul id="5b499afc-8928-4391-8ac7-41aa3111ca93" class="bulleted-list"><li style="list-style-type:disc">Pandas</li></ul><ul id="cd1b006f-e9d8-4e03-9905-cd7002467bda" class="bulleted-list"><li style="list-style-type:disc">os</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0f7385a5-11e4-4fae-888c-9995ef4524c6" class="code"><code class="language-Python"># Import libraries
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd
import os</code></pre><p id="e5d9de07-b88a-430d-a78b-b74dda027234" class="">
</p><p id="6554ab61-3400-4cbf-ba38-dd4e5895e867" class="">และทำการ Set directory สำหรับการรัน Webdriver และสามารถใช้คำสั่งเพื่อรันเว็บไซต์ได้เลย<br/>(สามารถศึกษาเพิ่มเติมเกี่ยวกับ Webdriver ได้ที่ <br/><a href="https://chromedriver.chromium.org/downloads">https://chromedriver.chromium.org</a>)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a9942421-6658-454a-9d2b-e942300a9d69" class="code"><code class="language-Python"># Set directory
path = &quot;../Webdriver_folder/&quot; #กำหนด folder ที่จะให้ระบบทำงาน
os.chdir(path) #รัน Code เพื่อสั่งให้ระบบทำงานบน folder ที่ต้องการ

# URL of the webpage to scrape
url = &#x27;https://th.indeed.com/jobs?q=data+analyst&amp;l=Thailand&amp;from=searchOnHP&amp;vjk=f436453e1ab0b79f&#x27;

# Initialize a Chrome WebDriver instance 
driver = webdriver.Chrome(path+&quot;chromedriver.exe&quot;)
driver.get(url)</code></pre><p id="7cb37e0f-4df3-4b7a-aae5-056b94440ad5" class="">
</p><p id="f8ce9425-660b-4ae3-8246-c10132c84077" class="">จากนั้นเราจะใช้คำสั่ง<strong> .page_sourse</strong> เพื่อดึง HTML ทั้งหมดมาเก็บไว้ในตัวแปล html </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="7862fdbd-3193-4831-a381-54264152e5a5" class="code"><code class="language-Python"># Get the HTML source code of the page after it has fully loaded
html = driver.page_source</code></pre><p id="3047b549-d128-44e9-9775-840769d0eb80" class="">
</p><p id="38fe6ee5-54a2-434e-801d-825a2efee591" class="">ใช้คำสั่ง bs4.BeautifulSoup เพื่อแปลงข้อมูลจาก String object เป็น BeautifulSoup object และดึงข้อมูลทุกรายการบนหน้าเว็บโดยเริ่มจากกำหนดรายละเอียดของ Tag ให้ชัดเจน ด้วยคำสั่ง <strong>.find_all</strong></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="edabd773-dd8a-42b3-a2e3-16fc8119d493" class="code"><code class="language-Python"># Parse the HTML using BeautifulSoup
soup = BeautifulSoup(html, &#x27;html.parser&#x27;)

# Find all job listings on the page
job_listings = soup.find_all(&#x27;div&#x27;, class_=&#x27;job_seen_beacon&#x27;)</code></pre><p id="34d7c43f-1d02-4630-99bd-589ebe8d0550" class="">
</p><p id="1d28275c-0da6-46c7-93f6-31aace3afcc0" class=""><strong>สร้าง empty list</strong> เพื่อใช้สำหรับเก็บค่าต่างๆที่เราจะดึงมาจากเว็บไซต์</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="67d80446-9e18-4d3f-9042-737b664d8b26" class="code"><code class="language-Python"># Initialize a list to store scraped records
records = []</code></pre><p id="21db6792-b4c6-4d99-b620-ab4841ff7c87" class="">
</p><p id="29a6fc71-7bb2-43cc-b935-7284c1eac2c8" class="">ในขึ้นตอนนี้เราจะทำการสร้าง Function เพื่อใช้สำหรับการดึงข้อมูลที่ต้องการ</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ff854ef8-a1d0-44d3-a59d-674d10576e3b" class="code"><code class="language-Python">def get_data(job_listing):
    # Extract job title
    title = job_listing.find(&quot;a&quot;).find(&quot;span&quot;).text.strip()
    
    # Extract company name if available, otherwise assign an empty string
    try:
        company = job_listing.find(&#x27;span&#x27;, class_=&#x27;css-92r8pb eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        company = &#x27;&#x27;
    
    # Extract job location if available, otherwise assign an empty string
    try:
        location  = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-1p0sjhy eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        location = &#x27;&#x27;
        
    # Extract salary information if available, otherwise assign an empty string
    try:
        salary  = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-1cvvo1b eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        salary = &#x27;&#x27;
    
    # Extract job type if available, otherwise assign an empty string
    try:
        job_type = job_listing.find(&#x27;div&#x27;, class_=&#x27;js-match-insights-provider-tvvxwd ecydgvn1&#x27;).text.strip()
    except AttributeError:
        job_type = &#x27;&#x27;
    
    # Extract date posted
    date_posted = job_listing.find(&#x27;span&#x27;, class_=&#x27;css-qvloho eu4oa1w0&#x27;).text.strip()
    
    # Extract job summary
    summary = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-9446fg eu4oa1w0&#x27;).text.strip()
    
    # Return a tuple containing all the extracted information
    return (title, company, location, salary, job_type, date_posted, summary)</code></pre><p id="c75e4143-7f99-4b9d-a648-7f0cf087a6c7" class="">
</p><p id="fef874bc-9e37-4a5c-b0d5-291a2063e448" class="">เมื่อได้ Function ที่ใช้สำหรับการดึงข้อมูลต่างๆแล้วแต่ Function นี้จะสามารถดึงข้อมูลได้เพียงครั้งละ 1 หน้าเท่านั้น เราจึงต้องดึงข้อมูลของหน้าถัดไปแล้วทำการใช้ While Loop เพื่อรันคำสั่งให้ครบทุกหน้า</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ce763d5f-3250-430c-a31d-991493785213" class="code"><code class="language-Python"># Loop to scrape data from multiple pages until there are no more pages available
while True:
    try:
        # Extract the URL of the next page if available
        url = &#x27;https://th.indeed.com/&#x27; + soup.find(&#x27;a&#x27;, {&#x27;aria-label&#x27;:&#x27;Next Page&#x27;}).get(&#x27;href&#x27;)
    except AttributeError:
        # If there are no more pages available, break the loop
        break
    
    # Open the next page in the browser
    driver.get(url)
    
    # Get the HTML source code of the next page
    html = driver.page_source
    
    # Parse the HTML of the next page using BeautifulSoup
    soup = BeautifulSoup(html, &#x27;html.parser&#x27;)
    
    # Find all job listings on the next page
    job_listings = soup.find_all(&#x27;div&#x27;, class_=&#x27;job_seen_beacon&#x27;) 

    # Iterate through each job listing on the page
    for job_listing in job_listings:
        # Extract data from the current job listing
        record = get_data(job_listing)
        
        # Append the extracted data to the records list
        records.append(record)</code></pre><p id="4ea9c600-acad-42c0-b25b-4788093d2d51" class="">
</p><p id="4931c6f3-31b5-46bb-b835-613cd765336e" class="">หลังจากจบ Loop แล้วเราสามารถปิดเว็บไซต์จำลองที่เราสร้างขึ้นมา ด้วยคำสั่ง <strong>driver.quit()</strong></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e15f2fc5-a092-4c7d-931b-f87907e41d7d" class="code"><code class="language-Python"># Close the Chrome WebDriver instance
driver.quit()</code></pre><p id="3274a53b-aa77-4cde-8a81-bb14bf1b73a7" class="">
</p><p id="14d1cbef-84ff-44ed-958d-8ccc22def4c5" class="">นำข้อมูลที่ได้จากการ Scraping มาสร้างเป็น Dataframe ด้วยคำสั่ง <strong>pd.Dataframe </strong></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="464b6f5d-8629-43bc-9b1a-3252c7ee3796" class="code"><code class="language-Python"># Convert list of records into a DataFrame
df = pd.DataFrame(records, columns=[&#x27;Title&#x27;, &#x27;Company&#x27;, &#x27;Location&#x27;, &#x27;Salary&#x27;, &#x27;Job Type&#x27;, &#x27;Date Posted&#x27;, &#x27;Summary&#x27;])</code></pre><p id="56585f35-aa45-47ad-af1c-8c5a43d1d750" class="">
</p><p id="5a0bd323-1724-4351-bd00-544a438ca9d6" class="">สุดท้ายเราจะได้ข้อมูลการเปิดรับสมัครงานตำแหน่ง “Data Analyst” ทั้งหมดที่อยู่ในรูปแบบ Dataframe </p><figure id="4ae8ddbf-4cae-46ed-9059-130f2b28cf9c" class="image"><a href="Web%20Scraping%20Project%20(Indeed)%2051adf6417f8e4fd08a03583386f9831c/d650a7cd-3f0d-46a1-a9f0-740e360b5e09.png"><img style="width:960px" src="Web%20Scraping%20Project%20(Indeed)%2051adf6417f8e4fd08a03583386f9831c/d650a7cd-3f0d-46a1-a9f0-740e360b5e09.png"/></a></figure><p id="ef5bee9d-6936-4bfb-afc9-0482147d6420" class="">และสามารถ Export ออกไปเป็นไฟล์ที่ต้องการจำใช้ต่อได้เลยครับ </p><p id="46597b2f-d7a6-4c1d-88f0-b794422df815" class="">ตัวอย่างการ Export เป็น Excel</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8757c1de-4b5d-4cca-ad1f-7cde3a1b81ce" class="code"><code class="language-Python"># Save DataFrame to a CSV file
df.to_csv(&#x27;indeed_job_data.csv&#x27;, index=False)</code></pre><p id="19f8d86b-36b8-4897-b993-e145ddd56294" class="">
</p><p id="2ed0f1f8-b7ef-4f3d-9b5b-8ca47df9090f" class=""><strong>ข้างล่างนี้จะเป็นการรวบ Code ทั้งหมดในโปรเจคนี้ เพื่องานต่อการ Copy ครับ</strong></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d9b00025-cba7-4497-ac44-bc2bff8fb960" class="code"><code class="language-Python"># Import libraries
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd
import os

# Set directory
path = &quot;../Webdriver_folder/&quot; #กำหนด folder ที่จะให้ระบบทำงาน
os.chdir(path) #รัน Code เพื่อสั่งให้ระบบทำงานบน folder ที่ต้องการ

# URL of the webpage to scrape
url = &#x27;https://th.indeed.com/jobs?q=data+analyst&amp;l=Thailand&amp;from=searchOnHP&amp;vjk=f436453e1ab0b79f&#x27;

# Initialize a Chrome WebDriver instance 
driver = webdriver.Chrome(path+&quot;chromedriver.exe&quot;)
driver.get(url)

# Get the HTML source code of the page after it has fully loaded
html = driver.page_source

# Parse the HTML using BeautifulSoup
soup = BeautifulSoup(html, &#x27;html.parser&#x27;)

# Find all job listings on the page
job_listings = soup.find_all(&#x27;div&#x27;, class_=&#x27;job_seen_beacon&#x27;)

# Initialize a list to store scraped records
records = []

def get_data(job_listing):
    # Extract job title
    title = job_listing.find(&quot;a&quot;).find(&quot;span&quot;).text.strip()
    
    # Extract company name if available, otherwise assign an empty string
    try:
        company = job_listing.find(&#x27;span&#x27;, class_=&#x27;css-92r8pb eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        company = &#x27;&#x27;
    
    # Extract job location if available, otherwise assign an empty string
    try:
        location  = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-1p0sjhy eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        location = &#x27;&#x27;
        
    # Extract salary information if available, otherwise assign an empty string
    try:
        salary  = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-1cvvo1b eu4oa1w0&#x27;).text.strip()
    except AttributeError:
        salary = &#x27;&#x27;
    
    # Extract job type if available, otherwise assign an empty string
    try:
        job_type = job_listing.find(&#x27;div&#x27;, class_=&#x27;js-match-insights-provider-tvvxwd ecydgvn1&#x27;).text.strip()
    except AttributeError:
        job_type = &#x27;&#x27;
    
    # Extract date posted
    date_posted = job_listing.find(&#x27;span&#x27;, class_=&#x27;css-qvloho eu4oa1w0&#x27;).text.strip()
    
    # Extract job summary
    summary = job_listing.find(&#x27;div&#x27;, class_=&#x27;css-9446fg eu4oa1w0&#x27;).text.strip()
    
    # Return a tuple containing all the extracted information
    return (title, company, location, salary, job_type, date_posted, summary)
    
    # Loop to scrape data from multiple pages until there are no more pages available
while True:
    try:
        # Extract the URL of the next page if available
        url = &#x27;https://th.indeed.com/&#x27; + soup.find(&#x27;a&#x27;, {&#x27;aria-label&#x27;:&#x27;Next Page&#x27;}).get(&#x27;href&#x27;)
    except AttributeError:
        # If there are no more pages available, break the loop
        break
    
    # Open the next page in the browser
    driver.get(url)
    
    # Get the HTML source code of the next page
    html = driver.page_source
    
    # Parse the HTML of the next page using BeautifulSoup
    soup = BeautifulSoup(html, &#x27;html.parser&#x27;)
    
    # Find all job listings on the next page
    job_listings = soup.find_all(&#x27;div&#x27;, class_=&#x27;job_seen_beacon&#x27;) 

    # Iterate through each job listing on the page
    for job_listing in job_listings:
        # Extract data from the current job listing
        record = get_data(job_listing)
        
        # Append the extracted data to the records list
        records.append(record)
        
# Convert list of records into a DataFrame
df = pd.DataFrame(records, columns=[&#x27;Title&#x27;, &#x27;Company&#x27;, &#x27;Location&#x27;, &#x27;Salary&#x27;, &#x27;Job Type&#x27;, &#x27;Date Posted&#x27;, &#x27;Summary&#x27;])


# Save DataFrame to a CSV file
df.to_csv(&#x27;indeed_job_data.csv&#x27;, index=False)

print(&quot;Data saved to job_data.csv&quot;)</code></pre><p id="4a6eed8a-c639-46c4-87f3-c0c5024af8e6" class=""><strong>                                                                                                                                       Writed by : Akkarawin Saiprapakorn</strong></p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>